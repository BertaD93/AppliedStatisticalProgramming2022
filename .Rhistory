rsqr #R square
summary(lm_1)
#Create new variables for models
df$agesq <- df$age^2 #Age square
df$srvlngq <- df$srvlng^2 #Length of service squared
#Create factor variables
df$rgroupf <- factor(df$rgroup)
df$regionf <- factor(df$region)
#unique(df$regionf)
#unique(df$rgroupf)
#Create dummy variables to recreate models
#Factors <- dummy
for (i in 2:9) {
regiond <- paste0("regiondummy_",i)
df[,regiond] <- ifelse(df$regionf == i, 1,0)
}
for (i in 1:4) {
rgroupd <- paste0("rgroupdummy_",i)
df[,rgroupd] <- ifelse(df$rgroupf == i, 1,0)
}
#df[1:10,c("regionf", "regiondummy_2")]
#df[1:10,c("rgroupf", "rgroupdummy_4")]
#####First Model#####
lm_1 <- lm(nowtot ~ ngirls, df)
summary(lm_1)
#####Second Model#####
lm_2 <- lm(nowtot ~ ngirls+ totchi+ female+ white+ repub+age+ agesq+ demvote, df)
summary(lm_2)
#####Third Model#####
lm_3 <- lm(nowtot ~ ngirls+ totchi+ female+ white+ repub+age+ agesq+ demvote+srvlng+srvlngq+rgroupf+regionf, df)
summary(lm_3)
#####First Model#####
df_1 <- df[,c("nowtot","ngirls")]
mat_1 <- data.matrix(na.omit(df_1))
X <- mat_1[,"ngirls"]
Y <- mat_1[,"nowtot"]
X <- (cbind(1, X))
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y
bhat #Coefficients
yhat <- X %*% bhat
ehat <- Y - yhat
vhat <- ((t(ehat) %*% ehat)[1]/ (dim(X)[[1]]-dim(X)[[2]])) *  (solve(t(X) %*% X))
vhat
std_err <- sqrt(diag(vhat))
std_err #Standard errors
rsqr <- 1-(((t(ehat) %*% ehat)[1])/430)/(mean((Y-mean(Y))^2))
rsqr #R square
summary(lm_1)
#####Second Model#####
df_2 <- df[,c("nowtot","ngirls", "totchi", "female", "white", "repub", "age", "agesq", "demvote")]
mat_2 <- data.matrix(na.omit(df_2))
X <- mat_2[,c("ngirls", "totchi", "female", "white", "repub", "age", "agesq", "demvote")]
Y <- mat_2[,"nowtot"]
X <- (cbind(1, X))
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y
bhat #Coefficients
yhat <- X %*% bhat
ehat <- Y - yhat
vhat <- ((t(ehat) %*% ehat)[1]/ (dim(X)[[1]]-dim(X)[[2]])) *  (solve(t(X) %*% X))
vhat
std_err <- sqrt(diag(vhat))
std_err #Standard errors
rsqr <- 1-(((t(ehat) %*% ehat)[1])/430)/(mean((Y-mean(Y))^2))
rsqr #R square
summary(lm_2)
#####Third Model#####
df_3 <- df[,c("nowtot","ngirls", "totchi", "female", "white", "repub","age","agesq", "demvote","srvlng","srvlngq","regiondummy_2", "regiondummy_3", "regiondummy_4", "regiondummy_5", "regiondummy_6", "regiondummy_7", "regiondummy_8", "regiondummy_9","rgroupdummy_1", "rgroupdummy_2", "rgroupdummy_3", "rgroupdummy_4")]
mat_3 <- data.matrix(na.omit(df_3))
X <- mat_3[,c("ngirls", "totchi", "female", "white", "repub","age","agesq", "demvote","srvlng","srvlngq","regiondummy_2", "regiondummy_3", "regiondummy_4", "regiondummy_5", "regiondummy_6", "regiondummy_7", "regiondummy_8", "regiondummy_9","rgroupdummy_1", "rgroupdummy_2", "rgroupdummy_3", "rgroupdummy_4")]
Y <- mat_3[,"nowtot"]
X <- (cbind(1, X))
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y
bhat #Coefficients
yhat <- X %*% bhat
ehat <- Y - yhat
vhat <- ((t(ehat) %*% ehat)[1]/ (dim(X)[[1]]-dim(X)[[2]])) *  (solve(t(X) %*% X))
vhat
std_err <- sqrt(diag(vhat))
std_err #Standard errors
rsqr <- 1-(((t(ehat) %*% ehat)[1])/430)/(mean((Y-mean(Y))^2))
rsqr #R square
summary(lm_3)
df_1 <- df[,c("nowtot","ngirls")]
mat_1 <- data.matrix(na.omit(df_1))
X <- mat_1[,"ngirls"]
Y <- mat_1[,"nowtot"]
X <- (cbind(1, X))
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y
bhat #Coefficients
yhat <- X %*% bhat
ehat <- Y - yhat
vhat <- ((t(ehat) %*% ehat)[1]/ (dim(X)[[1]]-dim(X)[[2]])) *  (solve(t(X) %*% X))
vhat
std_err <- sqrt(diag(vhat))
std_err #Standard errors
rsqr <- 1-(((t(ehat) %*% ehat)[1])/430)/(mean((Y-mean(Y))^2))
rsqr #R square
summary(lm_1)
dim(X)
df_1 <- df[,c("nowtot","ngirls")]
mat_1 <- data.matrix(na.omit(df_1))
X <- mat_1[,"ngirls"]
Y <- mat_1[,"nowtot"]
X <- (cbind(1, X))
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y
bhat #Coefficients
yhat <- X %*% bhat
ehat <- Y - yhat
vhat <- ((t(ehat) %*% ehat)[1]/ (dim(X)[[1]]-dim(X)[[2]])) *  (solve(t(X) %*% X))
vhat
std_err <- sqrt(diag(vhat))
std_err #Standard errors
rsqr <- 1-(((t(ehat) %*% ehat)[1])/dim(X)[[1]])/(mean((Y-mean(Y))^2))
rsqr #R square
summary(lm_1)
df_2 <- df[,c("nowtot","ngirls", "totchi", "female", "white", "repub", "age", "agesq", "demvote")]
mat_2 <- data.matrix(na.omit(df_2))
X <- mat_2[,c("ngirls", "totchi", "female", "white", "repub", "age", "agesq", "demvote")]
Y <- mat_2[,"nowtot"]
X <- (cbind(1, X))
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y
bhat #Coefficients
yhat <- X %*% bhat
ehat <- Y - yhat
vhat <- ((t(ehat) %*% ehat)[1]/ (dim(X)[[1]]-dim(X)[[2]])) *  (solve(t(X) %*% X))
vhat
std_err <- sqrt(diag(vhat))
std_err #Standard errors
rsqr <- 1-(((t(ehat) %*% ehat)[1])/dim(X)[[1]])/(mean((Y-mean(Y))^2))
rsqr #R square
summary(lm_2)
df_3 <- df[,c("nowtot","ngirls", "totchi", "female", "white", "repub","age","agesq", "demvote","srvlng","srvlngq","regiondummy_2", "regiondummy_3", "regiondummy_4", "regiondummy_5", "regiondummy_6", "regiondummy_7", "regiondummy_8", "regiondummy_9","rgroupdummy_1", "rgroupdummy_2", "rgroupdummy_3", "rgroupdummy_4")]
mat_3 <- data.matrix(na.omit(df_3))
X <- mat_3[,c("ngirls", "totchi", "female", "white", "repub","age","agesq", "demvote","srvlng","srvlngq","regiondummy_2", "regiondummy_3", "regiondummy_4", "regiondummy_5", "regiondummy_6", "regiondummy_7", "regiondummy_8", "regiondummy_9","rgroupdummy_1", "rgroupdummy_2", "rgroupdummy_3", "rgroupdummy_4")]
Y <- mat_3[,"nowtot"]
X <- (cbind(1, X))
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y
bhat #Coefficients
yhat <- X %*% bhat
ehat <- Y - yhat
vhat <- ((t(ehat) %*% ehat)[1]/ (dim(X)[[1]]-dim(X)[[2]])) *  (solve(t(X) %*% X))
vhat
std_err <- sqrt(diag(vhat))
std_err #Standard errors
rsqr <- 1-(((t(ehat) %*% ehat)[1])/dim(X)[[1]])/(mean((Y-mean(Y))^2))
rsqr #R square
summary(lm_3)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tm)
library(lubridate)
library(wordcloud)
library(readr)
trump_tweets_url <- 'https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv'
tweets <- read_csv(trump_tweets_url)
View(tweets)
#1
tweets %>% separate(created_at, into = c("cases", "population"))
#1
a <- tweets %>% separate(created_at, into = c("date", "time"), sep = " ")
View(a)
#1
a <- tweets %>% separate(created_at, into = c(as.Date("date", format="%m/%d/%Y"), "time"), sep = " ")
#1
a <- tweets %>% separate(created_at, into = c("date", "time"), sep = " ")
#1
a <- tweets %>% separate(created_at, into = c("date", "time"), sep = " ") %>% as.Date(date, format="%m/%d/%Y")
a <- tweets %>% separate(created_at, into = c("date", "time"), sep = " ") %>% as.Date(tweets$date, format="%m/%d/%Y")
" ") %>% as.Date(c_date, "%m/%d/%Y")
a <- tweets %>% separate(created_at, into = c("c_date", "time"), sep = " ") %>% as.Date(c_date, "%m/%d/%Y")
as.Date(c_date, "%m/%d/%Y")
as.Date(tweets$c_date, "%m/%d/%Y")
a <- tweets %>% separate(created_at, into = c("c_date", "time"), sep = " ")
as.Date(tweets$c_date, "%m/%d/%Y")
#1
a <- tweets %>% separate(created_at, into = c("c_date", "time"), sep = " ") %>% mutate(dates= as.Date(tweets$c_date, "%m/%d/%Y"))
a <- tweets %>% separate(created_at, into = c("c_date", "time"), sep = " ") %>% mutate(dates= as.Date(c_date, "%m/%d/%Y"))
s_tweets <- tweets %>% separate(created_at, into = c("date", "time"), sep = " ") %>% mutate(dates = as.Date(c_date, "%m/%d/%Y"))
s_tweets <- tweets %>% separate(created_at, into = c("sdate", "stime"), sep = " ") %>% mutate(dates = as.Date(sdate, "%m/%d/%Y"))
View(s_tweets)
#Me falta reportar el rango
#2
top5 <- s_tweets %>% filter(is_retweet==FALSE)
View(top5)
#Me falta reportar el rango
#2
top5 <- s_tweets %>% filter(!is_retweet)
#Me falta reportar el rango
#2
top5 <- s_tweets %>% filter(is_retweet==FALSE)
top5 <- s_tweets %>% filter(is_retweet==FALSE) %>% slice_max(retweet_count, n = 2)
#Me falta reportar el rango
#2
top5 <- s_tweets %>% filter(is_retweet==FALSE) %>% slice_max(order_by = retweet_count+ favorite_count, n = 5)
top5_1 <- s_tweets %>% filter(is_retweet==FALSE) %>% slice_max(favorite_count, n = 5)
View(top5_1)
top5_rt <- s_tweets %>% filter(is_retweet==FALSE) %>% slice_max( retweet_count, n = 5)
top5_fav <- s_tweets %>% filter(is_retweet==FALSE) %>% slice_max(favorite_count, n = 5)
top_fav <- s_tweets %>%
slice_max(favorite_count, n=5) %>%
select(favorite_count)
# Use slice_max() to get a vector of the top 5 retweeted numbers
top_rt <- s_tweets %>%
slice_max(retweet_count, n=5) %>%
select(retweet_count)
# Use the match function to identify rows belonging to a top favorited or top retweeted tweet.
# Select the text of these tweets.
s_tweets %>%
filter(retweet_count %in% top_rt$retweet_count | favorite_count %in% top_fav$favorite_count) %>%
select(text)
a <- s_tweets %>%
filter(retweet_count %in% top_rt$retweet_count | favorite_count %in% top_fav$favorite_count) %>%
select(text)
View(a)
vignette("tm")
View(s_tweets)
Corpus_trump <- VCorpus(VectorSource(s_tweets$text))
View(Corpus_trump)
inspect(Corpus_trump[[1]])
#4
Corpus_trump  <- Corpus_trump %>%  tm_map(stripWhitespace)
inspect(Corpus_trump[[1]])
#4
Corpus_trump  <- Corpus_trump %>%  tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, stopwords("english"))
inspect(Corpus_trump[[1]])
?tm_map
#4
Corpus_trump  <- Corpus_trump %>%  tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(removeNumbers)
Corpus_trump  <- Corpus_trump %>%  tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation)
inspect(Corpus_trump[[1]])
inspect(Corpus_trump[[2]])
View(Corpus_trump)
set.seed(1234) # for reproducibility
wordcloud(Corpus_trump, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
tdm <- TermDocumentMatrix(Corpus_trump, control = list(weighting
= weightTfIdf))
View(tdm)
inspect(tdm)
inspect(removeSparseTerms(dtm, 0.999))
inspect(removeSparseTerms(tdm, 0.999))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tm)
library(lubridate)
library(wordcloud)
library(readr)
trump_tweets_url <- 'https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv'
tweets <- read_csv(trump_tweets_url)
#Use the separate function to separate the two values in two variables. Then use the as.Date function to indicate is a date
s_tweets <- tweets %>% separate(created_at, into = c("sdate", "stime"), sep = " ") %>% mutate(dates = as.Date(sdate, "%m/%d/%Y"))
#Report the ranges of dates with summarise function and min and max arguments
s_tweets %>% summarise(min = min(dates),
max = max(dates))
#Use slice_max and filter to show the top 5 most popular and retweeted tweets.
top5_rt <- s_tweets %>% filter(is_retweet==FALSE) %>% slice_max(retweet_count, n = 5)
top5_fav <- s_tweets %>% filter(is_retweet==FALSE) %>% slice_max(favorite_count, n = 5)
knitr::kable(top5_rt$text)
knitr::kable(top5_fav$text)
s_tweets$text <- gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", s_tweets$text) #Remove URLS from the begining
#Use Vcorpus function to create a corpus of Trump's tweets
Corpus_trump <- VCorpus(VectorSource(s_tweets$text))
inspect(Corpus_trump[[1]])
Corpus_trump  <- Corpus_trump %>%  tm_map(stripWhitespace) %>% #remove whitespace
tm_map(content_transformer(tolower)) %>% #to lower case all letters
tm_map(removeWords, stopwords("english")) %>% #remove stop words
tm_map(removeNumbers) %>% #remove numbers
tm_map(removePunctuation) #remove punctuation
#Create a dtm
dtm <- TermDocumentMatrix(Corpus_trump, control = list(weighting= weightTfIdf))
dtm <- removeSparseTerms(dtm, .8)
View(dtm)
#Find the 50 words with highest tf.idf
dtm <- as.matrix(dtm)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm)
#Create a dtm
dtm <- TermDocumentMatrix(Corpus_trump, control = list(weighting= weightTfIdf))
dtm <- removeSparseTerms(dtm, .8)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm)
View(dtm_matrix)
dim(dtm_matrix)
#dtm <- removeSparseTerms(dtm, .8)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm)
dim(dtm_matrix)
#Create a dtm
dtm <- TermDocumentMatrix(Corpus_trump, control = list(weighting= weightTfIdf))
#dtm <- removeSparseTerms(dtm, .8)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm)
dtm_sparse <- removeSparseTerms(dtm, .20)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm_sparse)
dim(dtm_matrix)
dtm_sparse <- removeSparseTerms(dtm, .80)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm_sparse)
dim(dtm_matrix)
View(dtm_matrix)
dtm
insoect(dtm)
inspect(dtm)
dtm_sparse <- removeSparseTerms(dtm, .80)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm_sparse)
dim(dtm_matrix)
inspect(dtm_sparse)
dtm_sparse <- removeSparseTerms(dtm, .99)
dtm_sparse <- removeSparseTerms(dtm, .99)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm_sparse)
dim(dtm_matrix)
inspect(dtm_sparse)
dtm_matrix
which(m == max(dtm_matrix), arr.ind = TRUE)
which(dtm_matrix == max(dtm_matrix), arr.ind = TRUE)
dtm_sparse <- removeSparseTerms(dtm, .999999)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm_sparse)
dtm_sparse <- removeSparseTerms(dtm, .9)
inspect(dtm_sparse)
dtm_sparse <- removeSparseTerms(dtm, .999)
inspect(dtm_sparse)
dtm_sparse <- removeSparseTerms(dtm, .999)
inspect(dtm_sparse)
#Find the 50 words with highest tf.idf
#dtm_matrix <- as.matrix(dtm_sparse)
#dim(dtm_matrix)
dtm_sparse <- removeSparseTerms(dtm, .99)
inspect(dtm_sparse)
#Find the 50 words with highest tf.idf
#dtm_matrix <- as.matrix(dtm_sparse)
#dim(dtm_matrix)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm_sparse)
dim(dtm_matrix)
dtm_matrix
dtm_matrix[1,1]
colnames(dtm_matrix)
rownames(dtm_matrix)
suma <- colSums(dtm_matrix)
suma
topwords <- tidy(dtm_matrix)
topwords <- data.frame(dtm_matrix)
View(topwords)
topwords <- data.frame(word=rownames(dtm_matrix), score=colSums(dtm_matrix))
a <- rownames(dtm_matrix)
a <- rownames(dtm_matrix)
b <- colSums(dtm_matrix)
topwords <- data.frame(word=a, score=b)
rownames(dtm_matrix)
dtm_matrix
dtm_matrix[1:2,1:2]
b <- rowSums(dtm_matrix)
topwords <- data.frame(word=a, score=b)
View(topwords)
dtm_sparse <- removeSparseTerms(dtm, .8)
inspect(dtm_sparse)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm_sparse)
dim(dtm_matrix)
a <- rownames(dtm_matrix)
b <- rowSums(dtm_matrix)
topwords <- data.frame(word=a, score=b) %>%  slice_max(score, n =50)
View(topwords)
dtm_sparse <- removeSparseTerms(dtm, .99)
inspect(dtm_sparse)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm_sparse)
dim(dtm_matrix)
a <- rownames(dtm_matrix)
b <- rowSums(dtm_matrix)
topwords <- data.frame(word=a, score=b) %>% slice_max(score, n =50)
View(topwords)
length(Corpus_trump)
#Create a dtm
dtm <- TermDocumentMatrix(Corpus_trump, control = list(weighting= weightTfIdf, global=c(1,length(Corpus_trump)*.80)))
dtm_sparse <- removeSparseTerms(dtm, .99)
inspect(dtm_sparse)
#Find the 50 words with highest tf.idf
dtm_matrix <- as.matrix(dtm_sparse)
dim(dtm_matrix)
#Sum all the words throughout the documents and in this sense, we can calculate the 50 top words
topwords <- data.frame(word=rownames(dtm_matrix), score=rowSums(dtm_matrix)) %>% slice_max(score, n =50) #Top 50 words
knitr::kable(topwords)
8.5/25+8.5/30
pnorm(-1.5/sqrt(0.62))
0.02838991*2
(8.25/25)+(8.25/30)
8.25/25
8.25/30
0.33+.275
setClass(Class="Squares",
representation = representation(
square = "numeric",
x = "numeric",
y = "numeric"
),
prototype = prototype(
square = c(),
x = c(),
y = c()
)
)
setClass(Class="Squares",
representation = representation(
square = "numeric",
x = "numeric",
y = "numeric"
),
prototype = prototype(
square = c(),
x = c(),
y = c()
)
)
setValidity("Squares", function(object){
added<-object@x^2+object@y^2
subtracted<-object@x^2-object@y^2
test1<-all(object@square==added)
test2<-all(object@square==subtracted)
object@square!=added
if(!test1 & !test2){return("@square is not a valid value")}
}
)
setMethod("initialize", "Squares",  function(.Object, ...) {
value = callNextMethod()
validObject(value)
return(value)
})
setGeneric("getSquares",
function(object="Squares")  {
standardGeneric("getSquares")
} )
setMethod("getSquares", "Squares",
function(object){
return(object@square)
} )
# Usage
myObject<-new("Squares", square=13, x=3, y=2)
getSquares(myObject)
setGeneric("setsquare<-",
function(object, value)  {
standardGeneric("setsquare<-")
} )
setReplaceMethod(
f="setsquare",
signature="Squares",
definition=function(object,value){
object@square<-value
validObject(object) # Check validity before returning
return(object)
}
)
## Example
setsquare(myObject)<-13
setsquare(myObject)<-14
?pnorm()
pnorm(.60)
pnorm(1/sqrt(.605))
pnorm(sqrt(.605))
(1.5/sqrt(.605))
pnorm(1.928473)
0.9731018*2
-1.5/sqrt(.605)
pnorm(-1.928473)
pnorm(-1.5/sqrt(.605))
knitr::opts_chunk$set(echo = TRUE)
#We use pnorm function to calculate the proability. Then the result, we multiply for two to have the two sides.
pr <- pnorm(-1.5/sqrt(.605))
pr <- pr*2
pr
0.02689816*2
Sys.getenv("PATH")
library(devtools)
library(roxygen2)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
## This is run once when the package strcuture is first created
## This can be run many times as the code is updates
current.code <- as.package("integrateIt")
load_all(current.code)
document(current.code)
x = 1:10
f = 1:10
limits = c(1,10)
integrateIt(x, f, limits, "Simpson")
integrateIt(x, f, limits, "Trapezoid")
fun = function(x) sin(x)
tolerance = 0.001
rule = "Trapezoid"
start = 10
limits = c(1,10)
tolTest(fun=fun, tolerance=tolerance, rule=rule, start=start, limits=limits)
step <- (10 - 5) / 6
x <- seq(10,5 , by = step)
x <- seq(5,10 , by = step)
current.code <- as.package("integrateIt")
load_all(current.code)
document(current.code)
current.code <- as.package("integrateIt")
load_all(current.code)
document(current.code)
